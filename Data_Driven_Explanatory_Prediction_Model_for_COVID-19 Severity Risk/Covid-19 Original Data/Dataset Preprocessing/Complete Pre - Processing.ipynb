{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xlsxwriter import Workbook\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'Dataset.xlsx'\n",
    "dataset = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an Index Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(inplace=True)\n",
    "dataset.rename(columns={'index': 'Index'}, inplace=True)\n",
    "dataset['Index'] = dataset['Index'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Index' column to 'Subject_ID' \n",
    "dataset = dataset.rename(columns={'Index': 'Subject_ID'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in the 'outcome' column\n",
    "missing_outcome_count = dataset['outcome'].isnull().sum()\n",
    "\n",
    "# Identify the rows with missing values in the 'outcome' column\n",
    "missing_outcome_rows = dataset[dataset['outcome'].isnull()]\n",
    "\n",
    "print(\"Number of missing values in 'outcome':\", missing_outcome_count)\n",
    "print(\"Rows with missing 'outcome' values:\\n\", missing_outcome_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for mapping the categories to numerical values\n",
    "outcome_mapping = {\n",
    "    'discharge': 1,\n",
    "    'death': 2,\n",
    "    'transfer': 3\n",
    "}\n",
    "\n",
    "# Mapping the categories to numbers\n",
    "dataset['Outcome_numerical'] = dataset['outcome'].map(outcome_mapping)\n",
    "\n",
    "# Display the first few rows to verify the transformation\n",
    "dataset[['outcome', 'Outcome_numerical']].head()\n",
    "\n",
    "# Dropping the original 'outcome' column\n",
    "dataset = dataset.drop('outcome', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    429\n",
       "2.0     55\n",
       "3.0      1\n",
       "Name: Outcome_numerical, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset['Outcome_numerical'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the gender column\n",
    "dataset['GENDER'] = dataset['GENDER'].replace({'Μ': 'M'})\n",
    "\n",
    "# Map the 'GENDER' column to numerical values: 'F' to 1 and 'M' to 2\n",
    "dataset['GENDER'] = dataset['GENDER'].map({'F': 1, 'M': 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Check the number of missing values in the original 'AGE' column\\nmissing_ages_original = dataset['AGE'].isnull().sum()\\n\\n# Impute missing values in the 'AGE' column with the median age before categorization\\nmedian_age = dataset['AGE'].median()\\ndataset['AGE'] = dataset['AGE'].fillna(median_age)\\n\\n# Verify if all missing values are filled\\nmissing_ages_after_imputation = dataset['AGE'].isnull().sum()\\n\\n(missing_ages_original, missing_ages_after_imputation, median_age)\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate age from birth date\n",
    "def calculate_age(born):\n",
    "    today = datetime.today()\n",
    "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "\n",
    "# Apply the function to the 'AGE' column\n",
    "dataset['AGE'] = dataset['AGE'].apply(lambda x: calculate_age(x) if isinstance(x, datetime) else x)\n",
    "\"\"\"\n",
    "# Check the number of missing values in the original 'AGE' column\n",
    "missing_ages_original = dataset['AGE'].isnull().sum()\n",
    "\n",
    "# Impute missing values in the 'AGE' column with the median age before categorization\n",
    "median_age = dataset['AGE'].median()\n",
    "dataset['AGE'] = dataset['AGE'].fillna(median_age)\n",
    "\n",
    "# Verify if all missing values are filled\n",
    "missing_ages_after_imputation = dataset['AGE'].isnull().sum()\n",
    "\n",
    "(missing_ages_original, missing_ages_after_imputation, median_age)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOS column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIxing some values\n",
    "dataset['LOS'] = dataset['LOS'].replace('26+', 26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'DAYS OF SYMPTOMS' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'ΝΟ' with NaN in the 'DAYS OF SYMPTOMS' column\n",
    "dataset['DAYS OF SYMPTOMS'] = dataset['DAYS OF SYMPTOMS'].replace('ΝΟ', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFILTRATE COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bilateral', 'r>l', 'l', 'ok', 'r', 'l>r', 'missing', 'fluid l'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the function to correctly standardize the 'INFILTRATE' column from the beginning\n",
    "def standardize_infiltrate(value):\n",
    "    if pd.isnull(value):\n",
    "        return 'missing'  # Representing missing data\n",
    "    elif value in ['ΟΚ', 'OK']:\n",
    "        return 'ok'       # Representing no significant infiltrates\n",
    "    else:\n",
    "        return value.strip().lower()  # Standardize the rest of the values\n",
    "\n",
    "# Apply the redefined function to the 'INFILTRATE' column\n",
    "dataset['INFILTRATE'] = dataset['INFILTRATE'].apply(standardize_infiltrate)\n",
    "\n",
    "# Check the unique values after re-standardization\n",
    "dataset['INFILTRATE'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for mapping the categories to numerical values\n",
    "infiltrate_mapping = {\n",
    "    'bilateral': 1,\n",
    "    'r>l': 2,\n",
    "    'l': 3,\n",
    "    'ok': 4,\n",
    "    'r': 5,\n",
    "    'l>r': 6,\n",
    "    'fluid l': 7\n",
    "}\n",
    "\n",
    "# Mapping the categories to numbers\n",
    "dataset['INFILTRATE_numerical'] = dataset['INFILTRATE'].map(infiltrate_mapping)\n",
    "\n",
    "# Display the first few rows to verify the transformation\n",
    "dataset[['INFILTRATE', 'INFILTRATE_numerical']].head()\n",
    "\n",
    "# Dropping the original 'INFILTRATE' column\n",
    "dataset = dataset.drop('INFILTRATE', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symptoms columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the translation dictionary\n",
    "translation_dict = {\n",
    "    'ΚΕΦΑΛΑΛΓΙΑ': 'HEADACHE',\n",
    "    'ΑΝΟΣΜΙΑ': 'ANOSMIA',\n",
    "    'ΝΑΥΤΙΑ': 'NAUSEA',\n",
    "    'ΖΑΛΗ-ΑΣΤΑΘΕΙΑ-ΣΥΓΧΗΣΗ': 'DIZZINESS-INSTABILITY-CONFUSION',\n",
    "    'ΠΛΕΥΡΙΤΙΚΗ ΣΥΛΛΟΓΗ': 'PLEURAL EFFUSION',\n",
    "    'EMETOI': 'VOMITING'\n",
    "}\n",
    "\n",
    "# Define the symptom_columns list\n",
    "symptom_columns = ['FEVER', 'COUGH', 'FATIGUE', 'DIARRHEAS', 'DYSPNEA', 'URTI']\n",
    "\n",
    "# Step 1: Create columns for each symptom in symptom_columns\n",
    "for symptom in symptom_columns:\n",
    "    dataset[symptom + 'x'] = dataset[symptom].apply(lambda x: 1 if pd.notnull(x) and x else 0)\n",
    "\n",
    "# Step 2: Translate the 'OTHER' column and create columns for each translated symptom\n",
    "# Translate the 'OTHER' column\n",
    "dataset['Translated_Other'] = dataset['OTHER'].map(translation_dict)\n",
    "\n",
    "# Split the 'Translated_Other' into separate symptoms and create columns\n",
    "for translated_symptom in translation_dict.values():\n",
    "    dataset[translated_symptom] = dataset['Translated_Other'].apply(lambda x: 1 if x == translated_symptom else 0)\n",
    "\n",
    "# Dropping the old columns\n",
    "columns_to_drop = ['FEVER',\t'COUGH',\t'FATIGUE',\t'DIARRHEAS',\t'DYSPNEA',\t'URTI',\t'OTHER', 'Translated_Other']\n",
    "\n",
    "dataset = dataset.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comorbilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CM/CHF_Presence</th>\n",
       "      <th>CM/CAD_Presence</th>\n",
       "      <th>CM/AF_Presence</th>\n",
       "      <th>CM/HBP_Presence</th>\n",
       "      <th>CM/asthma_Presence</th>\n",
       "      <th>CM/COPD_Presence</th>\n",
       "      <th>CM/DM_Presence</th>\n",
       "      <th>DM REGISTRY_Presence</th>\n",
       "      <th>CM/neoplasm_Presence</th>\n",
       "      <th>CM/CNS_Presence</th>\n",
       "      <th>CM/GI_Presence</th>\n",
       "      <th>CM/renal_Presence</th>\n",
       "      <th>cm/autoimm_Presence</th>\n",
       "      <th>CM/lipid_Presence</th>\n",
       "      <th>CM/metabolic/other_Presence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CM/CHF_Presence  CM/CAD_Presence  CM/AF_Presence  CM/HBP_Presence  \\\n",
       "0                0                0               0                0   \n",
       "1                0                0               0                0   \n",
       "2                0                0               0                1   \n",
       "3                0                0               0                0   \n",
       "4                1                0               0                1   \n",
       "\n",
       "   CM/asthma_Presence  CM/COPD_Presence  CM/DM_Presence  DM REGISTRY_Presence  \\\n",
       "0                   0                 0               0                     0   \n",
       "1                   0                 0               0                     0   \n",
       "2                   0                 0               1                     1   \n",
       "3                   0                 0               0                     0   \n",
       "4                   0                 0               0                     0   \n",
       "\n",
       "   CM/neoplasm_Presence  CM/CNS_Presence  CM/GI_Presence  CM/renal_Presence  \\\n",
       "0                     0                0               0                  0   \n",
       "1                     0                0               0                  0   \n",
       "2                     0                0               0                  0   \n",
       "3                     0                0               0                  0   \n",
       "4                     1                0               0                  0   \n",
       "\n",
       "   cm/autoimm_Presence  CM/lipid_Presence  CM/metabolic/other_Presence  \n",
       "0                    0                  0                            0  \n",
       "1                    0                  0                            0  \n",
       "2                    0                  1                            1  \n",
       "3                    0                  0                            1  \n",
       "4                    0                  1                            0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the comorbidity columns list\n",
    "comorbidity_columns_updated = [\n",
    "    'CM/CHF', 'CM/CAD', 'CM/AF', 'CM/HBP', 'CM/asthma', 'CM/COPD', 'CM/DM',\n",
    "    'DM REGISTRY', 'CM/neoplasm', 'CM/CNS', 'CM/GI', 'CM/renal', \n",
    "    'cm/autoimm', 'CM/lipid', 'CM/metabolic/other', 'CM/other'\n",
    "]\n",
    "# Create columns for each comorbidity with 1 for presence and 0 for absence\n",
    "for comorbidity in comorbidity_columns_updated:\n",
    "    if comorbidity != 'CM/other':\n",
    "        dataset[comorbidity + '_Presence'] = dataset[comorbidity].apply(lambda x: 1 if pd.notnull(x) and x else 0)\n",
    "\n",
    "# Display the first few rows to verify the transformation for these comorbidities\n",
    "dataset[[comorbidity + '_Presence' for comorbidity in comorbidity_columns_updated if comorbidity != 'CM/other']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ΚΥΠ',\n",
       " 'Ψ',\n",
       " 'ΧΛΛ',\n",
       " 'ΗΒV',\n",
       " 'ΧΘΕΝ ΑΝΕΥΡΥΣΜΑ ΚΟΙΛΙΑΚΗΣ ΑΟΡΤΗΣ',\n",
       " 'ΛΕΜΦΩΜΑ',\n",
       " 'ΑΝΑΙΜΙΑ',\n",
       " 'ΒΑΛΒΙΔΟΠΛΑΣΤΙΚΗ ΑΟΡΤΙΚΗΣ',\n",
       " 'ΑΚΜΗ',\n",
       " 'HBV',\n",
       " 'ΟΥΡΙΧΑΙΜΙΑ',\n",
       " 'ΒΜΙ',\n",
       " 'ΑΛΚΟΟΛ',\n",
       " 'ΕΓΚΥΟΣ',\n",
       " 'ΑΘΗΡΩΜΑΤΩΣΗ ΚΑΡΩΤΙΔΩΝ',\n",
       " 'BMI',\n",
       " 'ΛΕΧΩΝΑ',\n",
       " 'PACEMAKER',\n",
       " 'ΙΔΙΟΠΑΘΗΣ ΤΡΟΜΟΣ',\n",
       " 'Β ΕΤΕΡΟΖΥΓΟΣ',\n",
       " 'ΝΕΦΡΟΛΙΘΙΑΣΗ',\n",
       " 'ΑΙΜΟΛΥΤΙΚΗ ΑΝΑΙΜΙΑ',\n",
       " 'ΑΓΚΥΛΟΠΟΙΗΤΙΚΗ ΣΠΟΝΔΥΛΙΤΙΔΑ',\n",
       " 'ΑΝΕΥΡΥΣΜΑ ΑΟΡΤΗΣ',\n",
       " 'MDS',\n",
       " 'ΠΟΛΛΑΠΛΟΥΝ ΜΥΕΛΩΜΑ',\n",
       " 'ΣΑΡΚΟΕΙΔΩΣΗ',\n",
       " 'ΠΕ',\n",
       " 'ΜΕΤΑΛΛΙΚΗ ΒΑΛΒΙΔΑ',\n",
       " 'ΕΤΕΡΟΖΥΓΟΣ V LEIDEN',\n",
       " 'TB',\n",
       " 'ΕΜΦΡΑΚΤΟ ΝΕΦΡΟΥ',\n",
       " 'ΣΤΕΝΩΣΗ ΚΑΡΩΤΙΔΩΝ',\n",
       " 'ΚΑΤΑΚΕΚΛΙΜΕΝΗ',\n",
       " 'ΠΑΛΙΝΔΡΟΜΟΣ ΚΥΗΣΗ',\n",
       " 'NON HODGKIN ',\n",
       " 'ΝΟΗΤΙΚΗ ΥΣΤΕΡΗΣΗ']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifying unique values in the 'CM/other' column\n",
    "unique_cm_other_values = dataset['CM/other'].dropna().unique()\n",
    "\n",
    "# Filtering out combinatory values and creating a list of individual terms\n",
    "individual_cm_other_values = [val for val in unique_cm_other_values if '/' not in val  and '-' not in val]\n",
    "\n",
    "individual_cm_other_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete translation dictionary for common medical abbreviations and terms in 'CM/other' column\n",
    "# This includes the translations we identified earlier and leaves untranslated terms as-is\n",
    "complete_medical_translation_dict = {\n",
    "    'ΚΥΠ': 'Chronic Kidney Disease',\n",
    "    'Ψ': 'Psychiatric Disorder',\n",
    "    'ΧΛΛ': 'Chronic Lymphocytic Leukemia',\n",
    "    'ΗΒV': 'Hepatitis B Virus',\n",
    "    'ΧΘΕΝ ΑΝΕΥΡΥΣΜΑ ΚΟΙΛΙΑΚΗΣ ΑΟΡΤΗΣ': 'Abdominal Aortic Aneurysm',\n",
    "    'ΛΕΜΦΩΜΑ': 'Lymphoma',\n",
    "    'ΑΝΑΙΜΙΑ': 'Anemia',\n",
    "    'ΒΑΛΒΙΔΟΠΛΑΣΤΙΚΗ ΑΟΡΤΙΚΗΣ': 'Aortic Valve Plasty',\n",
    "    'ΑΚΜΗ': 'Acne',\n",
    "    'HBV': 'Hepatitis B Virus',\n",
    "    'ΟΥΡΙΧΑΙΜΙΑ': 'Uricemia',\n",
    "    'ΒΜΙ': 'BMI',\n",
    "    'ΑΛΚΟΟΛ': 'Alcohol',\n",
    "    'ΕΓΚΥΟΣ': 'Pregnant',\n",
    "    'ΑΘΗΡΩΜΑΤΩΣΗ ΚΑΡΩΤΙΔΩΝ': 'Carotid Atherosclerosis',\n",
    "    'BMI': 'Body Mass Index',\n",
    "    'ΛΕΧΩΝΑ': 'Leukemia',\n",
    "    'PACEMAKER': 'Pacemaker',\n",
    "    'ΙΔΙΟΠΑΘΗΣ ΤΡΟΜΟΣ': 'Idiopathic Tremor',\n",
    "    'Β ΕΤΕΡΟΖΥΓΟΣ': 'Beta Thalassemia Heterozygous',\n",
    "    'ΝΕΦΡΟΛΙΘΙΑΣΗ': 'Nephrolithiasis',\n",
    "    'ΑΙΜΟΛΥΤΙΚΗ ΑΝΑΙΜΙΑ': 'Hemolytic Anemia',\n",
    "    'ΑΓΚΥΛΟΠΟΙΗΤΙΚΗ ΣΠΟΝΔΥΛΙΤΙΔΑ': 'Ankylosing Spondylitis',\n",
    "    'ΑΝΕΥΡΥΣΜΑ ΑΟΡΤΗΣ': 'Aortic Aneurysm',\n",
    "    'MDS': 'Myelodysplastic Syndrome',\n",
    "    'ΠΟΛΛΑΠΛΟΥΝ ΜΥΕΛΩΜΑ': 'Multiple Myeloma',\n",
    "    'ΣΑΡΚΟΕΙΔΩΣΗ': 'Sarcoidosis',\n",
    "    'ΠΕ': 'Pulmonary Embolism',\n",
    "    'ΜΕΤΑΛΛΙΚΗ ΒΑΛΒΙΔΑ': 'Metallic Valve',\n",
    "    'ΕΤΕΡΟΖΥΓΟΣ V LEIDEN': 'Heterozygous Factor V Leiden',\n",
    "    'TB': 'Tuberculosis',\n",
    "    'ΕΜΦΡΑΚΤΟ ΝΕΦΡΟΥ': 'Renal Infarction',\n",
    "    'ΣΤΕΝΩΣΗ ΚΑΡΩΤΙΔΩΝ': 'Carotid Stenosis',\n",
    "    'ΚΑΤΑΚΕΚΛΙΜΕΝΗ': 'Bedridden',\n",
    "    'ΠΑΛΙΝΔΡΟΜΟΣ ΚΥΗΣΗ': 'Recurrent Pregnancy',\n",
    "    'NON HODGKIN': 'Non-Hodgkin Lymphoma',\n",
    "    'ΝΟΗΤΙΚΗ ΥΣΤΕΡΗΣΗ': 'Intellectual Disability'\n",
    "}\n",
    "# Apply the translations to the unique values in 'CM/other'\n",
    "translated_cm_other_values = [complete_medical_translation_dict.get(val, val) for val in individual_cm_other_values]\n",
    "\n",
    "# Creating columns for each translated value in 'CM/other'\n",
    "for original_value, translated_value in zip(individual_cm_other_values, translated_cm_other_values):\n",
    "    # Creating a column with the translated name\n",
    "    column_name = translated_value.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "    dataset[column_name] = dataset['CM/other'].apply(lambda x: 1 if x == original_value else 0)\n",
    "# Dropping the old columns\n",
    "columns_to_drop = ['MED/cardio',\t'MED/statins',\t'MED/antiPLTs',\t'MED/anticoag',\t'MED/resp',\t'MED/endocr',\t'MED/DM',\t'MED/CNS',\t'MED/GI',\t'MED/bones',\t'MED/immuno',\t'MED/antineo',\t'MED/other',\t'CM/CHF',\t'CM/CAD',\t'CM/AF',\t'CM/HBP',\t'CM/asthma',\t'CM/COPD',\t'CM/DM',\t'DM REGISTRY',\t'CM/neoplasm',\t'CM/CNS',\t'CM/hepatobiliary',\t'CM/GI',\t'CM/renal',\t'cm/autoimm',\t'CM/lipid',\t'CM/metabolic/other',\t'CM/other',]\n",
    "\n",
    "dataset = dataset.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Medication_meronem</th>\n",
       "      <th>Medication_zebaxa</th>\n",
       "      <th>Medication_augmentin</th>\n",
       "      <th>Medication_minocin</th>\n",
       "      <th>Medication_zovirax_solucortef</th>\n",
       "      <th>Medication_ecalta</th>\n",
       "      <th>Medication_mefoxil</th>\n",
       "      <th>Medication_medrol</th>\n",
       "      <th>Medication_ambisome</th>\n",
       "      <th>Medication_rivaroxaban</th>\n",
       "      <th>...</th>\n",
       "      <th>Medication_zavicefta</th>\n",
       "      <th>Medication_vibramycin</th>\n",
       "      <th>Medication_begalin</th>\n",
       "      <th>Medication_fosfomycin</th>\n",
       "      <th>Medication_fondaparinux</th>\n",
       "      <th>Medication_colistin</th>\n",
       "      <th>Medication_tinzaparin</th>\n",
       "      <th>Medication_targocid</th>\n",
       "      <th>Medication_colchicine</th>\n",
       "      <th>Medication_tavanic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Medication_meronem  Medication_zebaxa  Medication_augmentin  \\\n",
       "0                   0                  0                     0   \n",
       "1                   0                  0                     0   \n",
       "2                   0                  0                     0   \n",
       "3                   0                  0                     0   \n",
       "4                   1                  0                     0   \n",
       "\n",
       "   Medication_minocin  Medication_zovirax_solucortef  Medication_ecalta  \\\n",
       "0                   0                              0                  0   \n",
       "1                   0                              0                  0   \n",
       "2                   0                              0                  0   \n",
       "3                   0                              0                  0   \n",
       "4                   1                              0                  0   \n",
       "\n",
       "   Medication_mefoxil  Medication_medrol  Medication_ambisome  \\\n",
       "0                   0                  0                    0   \n",
       "1                   0                  0                    0   \n",
       "2                   0                  0                    0   \n",
       "3                   0                  0                    0   \n",
       "4                   0                  0                    0   \n",
       "\n",
       "   Medication_rivaroxaban  ...  Medication_zavicefta  Medication_vibramycin  \\\n",
       "0                       0  ...                     0                      0   \n",
       "1                       0  ...                     0                      0   \n",
       "2                       0  ...                     0                      0   \n",
       "3                       0  ...                     0                      0   \n",
       "4                       0  ...                     0                      0   \n",
       "\n",
       "   Medication_begalin  Medication_fosfomycin  Medication_fondaparinux  \\\n",
       "0                   0                      0                        0   \n",
       "1                   0                      0                        0   \n",
       "2                   0                      0                        0   \n",
       "3                   0                      0                        0   \n",
       "4                   0                      0                        0   \n",
       "\n",
       "   Medication_colistin  Medication_tinzaparin  Medication_targocid  \\\n",
       "0                    0                      0                    0   \n",
       "1                    0                      0                    0   \n",
       "2                    0                      0                    0   \n",
       "3                    0                      0                    0   \n",
       "4                    0                      0                    0   \n",
       "\n",
       "   Medication_colchicine  Medication_tavanic  \n",
       "0                      0                   1  \n",
       "1                      0                   0  \n",
       "2                      0                   0  \n",
       "3                      0                   0  \n",
       "4                      0                   0  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-defining the medication_columns list as it was lost during the re-upload of the file\n",
    "medication_columns = [\n",
    "    'ANTIBIOTIC 1', 'ANTIBIOTIC 2', 'ANTIBIOTIC 3', 'ANTIBIOTIC 4', 'ANTIBIOTIC 5', \n",
    "    'ANTIBIOTIC 6', 'ANTIBIOTIC 7', 'ANTIBIOTIC 8', 'ANTIBIOTIC 9', 'ANTICOAGULANT', 'ANTIFUNGAL 1', 'ANTIFUNGAL 2', 'OTHER2'\n",
    "]\n",
    "\n",
    "# Identifying unique values in medication columns\n",
    "unique_medication_values = set()\n",
    "\n",
    "for col in medication_columns:\n",
    "    unique_medication_values.update(dataset[col].dropna().unique())\n",
    "\n",
    "# Function to standardize medication names\n",
    "def standardize_med_name(med_name, name_mapping):\n",
    "    return name_mapping.get(med_name, med_name)\n",
    "\n",
    "# Dictionary for mapping non-standard names to standard names\n",
    "name_mapping = {\n",
    "    'zirhromax': 'zithromax',\n",
    "    'collisitn': 'colistin',\n",
    "    'collisitin': 'colistin',\n",
    "    'colisitn': 'colistin',\n",
    "    'noradren/solumedrol': 'solumedrol',\n",
    "    'nivestin/zovirax/solucortef': 'zovirax/solucortef',\n",
    "    'vonvon': 'voncon',\n",
    "    'DEXATON': 'dexamethasone',  # Assuming Dexaton is dexamethasone\n",
    "    'collistin': 'colistin',\n",
    "    'ΑΛΛΕΡΓΙΑ': 'allergia'  # Translating from Greek\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Standardizing unique medication values\n",
    "standardized_unique_medication_values = set([standardize_med_name(med, name_mapping) for med in unique_medication_values])\n",
    "\n",
    "# Displaying the standardized unique medication values\n",
    "standardized_unique_medication_values\n",
    "\n",
    "# Creating new columns for each standardized medication value\n",
    "for med in standardized_unique_medication_values:\n",
    "    if med != 1:  # Skipping columns that already indicate presence with a value of 1\n",
    "        column_name = \"Medication_\" + str(med).replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "        dataset[column_name] = dataset[medication_columns].apply(lambda row: 1 if med in row.values else 0, axis=1)\n",
    "\n",
    "dataset = dataset.drop(columns=medication_columns)\n",
    "\n",
    "# Displaying the first few rows to verify the new columns\n",
    "dataset[[col for col in dataset.columns if col.startswith(\"Medication_\")]].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns for specified medications, replacing missing values with 0, and then dropping the old columns\n",
    "specified_meds = ['REMDESIVIR', 'TAMIFLU', 'KALETRA', 'DEXATON', 'BARITICINIB', 'ANAKINRA', \n",
    "                  'TOCILIZUMAB', 'PLAQUENIL']\n",
    "\n",
    "for med in specified_meds:\n",
    "    new_column_name = med + \"_Processed\"\n",
    "    dataset[new_column_name] = dataset[med].fillna(0)\n",
    "\n",
    "# Dropping the old medication columns\n",
    "dataset.drop(columns=specified_meds, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping specified columns from the dataset\n",
    "columns_to_drop = [ 'DAY OF ADMISSION', 'INTUBATION DATE', 'DATE CPAP', 'CPAP / INTUB', 'FA ΟΙΚΕΙ', 'ΑΝΤΙΒ ΑΓΩΓΗ ΝΟΣΟΚ', 'GSC2',\t'BP2',\t'BPM',\t'rr2',\t'TEMP6',\t'PH7',\t'PO28',\t'PCO29',\t'HCO310',\t'FIO2 eisagwgh11',\t'GSC22',\t'BP23',\t'BPM4',\t'rr55',\t'TEMP62',\t'PH73',\t'PO284',\t'PCO295',\t'HCO3106',\t'FIO2 eisagwgh117', 'Στήλη1',\t'Στήλη2',\t'Στήλη3', 'Στήλη4', 'Στήλη5', 'Unnamed: 147',\t'Unnamed: 148', 'CCI', 'PO2/FIO2', 'SOFA', 'APACHE', 'PSI']\n",
    "dataset.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column \"HIGH FLOW \" to \"HIGH FLOW\"\n",
    "dataset.rename(columns={\"HIGH FLOW \": \"HIGH FLOW\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row Index</th>\n",
       "      <th>Missing Values in Row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>507</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>510</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>353</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>250</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>282</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>285</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>309</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>319</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>338</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>231</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>508</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>305</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>509</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>503</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>502</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>505</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>220</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>323</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>234</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>230</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>229</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>228</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>227</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>226</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>254</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>215</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>219</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>216</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>255</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>262</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>224</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>251</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>249</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>232</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>222</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>218</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Row Index  Missing Values in Row\n",
       "507        507                     53\n",
       "510        510                     50\n",
       "353        353                     49\n",
       "250        250                     49\n",
       "282        282                     49\n",
       "285        285                     49\n",
       "309        309                     49\n",
       "318        318                     49\n",
       "319        319                     49\n",
       "338        338                     49\n",
       "274        274                     49\n",
       "243        243                     48\n",
       "231        231                     48\n",
       "508        508                     47\n",
       "237        237                     47\n",
       "305        305                     46\n",
       "499        499                     45\n",
       "500        500                     45\n",
       "509        509                     44\n",
       "503        503                     39\n",
       "502        502                     34\n",
       "505        505                     34\n",
       "221        221                     34\n",
       "220        220                     32\n",
       "239        239                     32\n",
       "323        323                     31\n",
       "238        238                     31\n",
       "246        246                     31\n",
       "234        234                     31\n",
       "253        253                     31\n",
       "230        230                     31\n",
       "229        229                     31\n",
       "228        228                     31\n",
       "227        227                     31\n",
       "226        226                     31\n",
       "254        254                     31\n",
       "215        215                     31\n",
       "219        219                     31\n",
       "216        216                     31\n",
       "255        255                     31\n",
       "235        235                     30\n",
       "262        262                     30\n",
       "224        224                     30\n",
       "251        251                     30\n",
       "249        249                     30\n",
       "247        247                     30\n",
       "245        245                     30\n",
       "240        240                     30\n",
       "232        232                     30\n",
       "222        222                     29\n",
       "218        218                     28"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values in each row\n",
    "missing_values_per_row = dataset.isnull().sum(axis=1)\n",
    "\n",
    "# Creating a DataFrame to display the number of missing values in each row\n",
    "row_missing_data_analysis = pd.DataFrame({\n",
    "    'Row Index': missing_values_per_row.index, \n",
    "    'Missing Values in Row': missing_values_per_row.values\n",
    "})\n",
    "\n",
    "# Sorting the DataFrame based on the number of missing values in descending order\n",
    "row_missing_data_analysis.sort_values(by='Missing Values in Row', ascending=False, inplace=True)\n",
    "\n",
    "# Displaying the top 10 rows with the most missing values\n",
    "row_missing_data_analysis.head(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(thresh=(dataset.shape[1]-30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t HIGH FLOW       98.301486\n",
       "tINTUBATION       92.781316\n",
       "tCPAP             88.535032\n",
       "CHOL              84.925690\n",
       "TGL               84.288747\n",
       "rr                78.131635\n",
       "HIGH FLOW         38.853503\n",
       "FERRITIN          17.622081\n",
       "FIBRINOGEN        17.197452\n",
       "TnI               12.738854\n",
       "D-DIMERS           9.978769\n",
       "CPAP               9.341826\n",
       "DIRECT BIL         7.006369\n",
       "BIL                5.307856\n",
       "INR                4.458599\n",
       "APTT               4.458599\n",
       "CRP                4.246285\n",
       "PCO2               3.609342\n",
       "HCO3               3.609342\n",
       "WBC                3.609342\n",
       "K                  3.397028\n",
       "PO2                2.972399\n",
       "ABSLYMPHOCYTES     2.760085\n",
       "LDH                2.760085\n",
       "PLT                2.547771\n",
       "NA                 2.547771\n",
       "Glu                2.335456\n",
       "CR                 2.335456\n",
       "SGOT               2.335456\n",
       "SGPT               2.335456\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of specified columns\n",
    "specified_columns = [\n",
    "    \"AGE\", \"GENDER\", \"LOS\", \"DAYS OF SYMPTOMS\", \"INTUBATION\", \"tINTUBATION\", \"CPAP\", \"tCPAP\", \n",
    "    \"HIGH FLOW\", \"t HIGH FLOW\", \"GSC\", \"BP\", \"PULSE RATE\", \"rr\", \"TEMP\", \"PH\", \"PO2\", \"PCO2\", \n",
    "    \"HCO3\", \"FIO2 eisagwgh\", \"WHO score\", \"WBC\", \"LDH\", \"CPK\", \"CRP\", \"FERRITIN\", \"PERNEUTROPHILS\", \n",
    "    \"PERLYMPHOCYTES\", \"ABSLYMPHOCYTES\", \"Hb\", \"PLT\", \"INR\", \"APTT\", \"FIBRINOGEN\", \"D-DIMERS\", \n",
    "    \"K\", \"NA\", \"Glu\", \"UREA\", \"CR\", \"BIL\", \"DIRECT BIL\", \"SGOT\", \"SGPT\", \"TnI\", \"CHOL\", \"TGL\"\n",
    "]\n",
    "\n",
    "# Calculate the percentage of missing values for the specified columns\n",
    "missing_values_percentage_specified = dataset[specified_columns].isnull().mean() * 100\n",
    "missing_values_percentage_specified.sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns 'CHOL', 'TGL', and 'rr'\n",
    "dataset = dataset.drop(columns=['CHOL', 'TGL', 'rr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values into 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the process to only replace missing values for columns that exist in the dataset\n",
    "columns_to_replace_missing = ['tINTUBATION', 'CPAP', 'tCPAP', 'HIGH FLOW', 't HIGH FLOW']\n",
    "existing_columns = dataset.columns\n",
    "\n",
    "# Replacing missing values with 0 in the existing specified columns\n",
    "for col in columns_to_replace_missing:\n",
    "    if col in existing_columns:\n",
    "        dataset[col] = dataset[col].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the temperature values using Subject ID\n",
    "dataset.loc[dataset['Subject_ID'] == 172, 'TEMP'] = 38.2\n",
    "dataset.loc[dataset['Subject_ID'] == 313, 'TEMP'] = 36.1\n",
    "\n",
    "# Correcting the PCO2 value for Subject ID 2\n",
    "dataset.loc[dataset['Subject_ID'] == 2, 'PCO2'] = 31.2\n",
    "\n",
    "# Setting the HCO3 value for Subject ID 305 as missing\n",
    "dataset.loc[dataset['Subject_ID'] == 305, 'HCO3'] = pd.NA\n",
    "\n",
    "# Setting the WHO score of 15 to missing value\n",
    "dataset.loc[dataset['WHO score'] == 15, 'WHO score'] = pd.NA\n",
    "\n",
    "# Setting the specified WBC values to missing for the given Subject IDs\n",
    "subject_ids_to_update = [40, 41, 42, 43, 44, 45, 46, 47]\n",
    "for subject_id in subject_ids_to_update:\n",
    "    dataset.loc[dataset['Subject_ID'] == subject_id, 'WBC'] = pd.NA\n",
    "\n",
    "# Setting the LDH value for Subject ID 479 to missing\n",
    "dataset.loc[dataset['Subject_ID'] == 479, 'LDH'] = pd.NA\n",
    "\n",
    "# Setting the CPK value for Subject ID 233 to missing\n",
    "dataset.loc[dataset['Subject_ID'] == 233, 'CPK'] = pd.NA\n",
    "\n",
    "# Setting the CRP value for Subject ID 364 to missing\n",
    "dataset.loc[dataset['Subject_ID'] == 364, 'CRP'] = pd.NA\n",
    "\n",
    "# Identifying the highest value of Hb\n",
    "highest_hb_row = dataset['Hb'].idxmax()\n",
    "\n",
    "# Setting this value to missing\n",
    "dataset.loc[highest_hb_row, 'Hb'] = pd.NA\n",
    "\n",
    "# Setting INR values over 10 to missing\n",
    "dataset.loc[dataset['INR'] > 10, 'INR'] = pd.NA\n",
    "\n",
    "# Setting APTT values over 200 to missing\n",
    "dataset.loc[dataset['APTT'] > 200, 'APTT'] = pd.NA\n",
    "\n",
    "# Identifying and setting the highest value of BIL to missing\n",
    "highest_bil_row = dataset['BIL'].idxmax()\n",
    "dataset.loc[highest_bil_row, 'BIL'] = pd.NA\n",
    "\n",
    "# Setting Glu values over 600 to missing\n",
    "dataset.loc[dataset['Glu'] > 600, 'Glu'] = pd.NA\n",
    "\n",
    "# Setting UREA values over 500 to missing\n",
    "dataset.loc[dataset['UREA'] > 500, 'UREA'] = pd.NA\n",
    "\n",
    "# Identifying rows with DIRECT BIL values over 10\n",
    "direct_bil_over_10 = dataset[dataset['DIRECT BIL'] > 10][['Subject_ID', 'DIRECT BIL']]\n",
    "\n",
    "# Setting the identified high DIRECT BIL values to missing\n",
    "for subject_id in direct_bil_over_10['Subject_ID']:\n",
    "    dataset.loc[dataset['Subject_ID'] == subject_id, 'DIRECT BIL'] = pd.NA\n",
    "\n",
    "# Identifying and setting the highest value of TnI (Troponin I) to missing\n",
    "highest_tni_row = dataset['TnI'].idxmax()\n",
    "dataset.loc[highest_tni_row, 'TnI'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying outliers for FERRITIN\n",
    "Q1_ferritin = dataset['FERRITIN'].quantile(0.25)\n",
    "Q3_ferritin = dataset['FERRITIN'].quantile(0.75)\n",
    "IQR_ferritin = Q3_ferritin - Q1_ferritin\n",
    "lower_bound_ferritin = Q1_ferritin - 1.5 * IQR_ferritin\n",
    "upper_bound_ferritin = Q3_ferritin + 1.5 * IQR_ferritin\n",
    "outlier_rows_ferritin = dataset[(dataset['FERRITIN'] < lower_bound_ferritin) | (dataset['FERRITIN'] > upper_bound_ferritin)]\n",
    "\n",
    "outlier_rows_ferritin[['Subject_ID', 'FERRITIN']]\n",
    "\n",
    "# Identifying the three highest values of FERRITIN\n",
    "three_highest_ferritin = outlier_rows_ferritin.nlargest(3, 'FERRITIN')\n",
    "\n",
    "# Setting these values to missing\n",
    "for subject_id in three_highest_ferritin['Subject_ID']:\n",
    "    dataset.loc[dataset['Subject_ID'] == subject_id, 'FERRITIN'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying and setting the highest value of FIBRINOGEN to missing\n",
    "dataset['FIBRINOGEN'] = pd.to_numeric(dataset['FIBRINOGEN'], errors='coerce')\n",
    "highest_fibrinogen_row = dataset['FIBRINOGEN'].idxmax(skipna=True)\n",
    "dataset.loc[highest_fibrinogen_row, 'FIBRINOGEN'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying outliers for K with values above 100\n",
    "outlier_rows_k_above_100 = dataset[dataset['K'] > 100][['Subject_ID', 'K']]\n",
    "\n",
    "# Identifying outliers for NA with values above 1000\n",
    "outlier_rows_na_above_1000 = dataset[dataset['NA'] > 1000][['Subject_ID', 'NA']]\n",
    "\n",
    "\n",
    "# Setting the identified outliers for K and NA to missing values\n",
    "subject_ids_k_outliers = outlier_rows_k_above_100['Subject_ID']\n",
    "subject_ids_na_outliers = outlier_rows_na_above_1000['Subject_ID']\n",
    "\n",
    "for subject_id in subject_ids_k_outliers:\n",
    "    dataset.loc[dataset['Subject_ID'] == subject_id, 'K'] = pd.NA\n",
    "\n",
    "for subject_id in subject_ids_na_outliers:\n",
    "    dataset.loc[dataset['Subject_ID'] == subject_id, 'NA'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the \"D-DIMERS\" column by converting non-standard entries like '15+' and '15+++' to numeric values (e.g., '15+'' to 15)\n",
    "# We will use a regular expression to extract the numeric part from these strings\n",
    "\n",
    "import re\n",
    "\n",
    "D_DIMERS_column_index = dataset.columns.get_loc('D-DIMERS')\n",
    "\n",
    "def clean_d_dimers(value):\n",
    "    if isinstance(value, str):\n",
    "        # Extract the numeric part from the string\n",
    "        match = re.search(r'\\d+', value)\n",
    "        return float(match.group()) if match else None\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "dataset['D-DIMERS_cleaned'] = dataset['D-DIMERS'].apply(clean_d_dimers)\n",
    "\n",
    "# Converting the cleaned column to numeric type\n",
    "dataset['D-DIMERS_cleaned'] = pd.to_numeric(dataset['D-DIMERS_cleaned'], errors='coerce')\n",
    "\n",
    "# Checking for missing values after cleaning and conversion\n",
    "cleaned_missing_values = dataset['D-DIMERS_cleaned'].isnull().sum()\n",
    "\n",
    "# Dropping the original 'D-DIMERS' column\n",
    "dataset.drop('D-DIMERS', axis=1, inplace=True)\n",
    "\n",
    "# Displaying basic descriptive statistics of the cleaned \"D-DIMERS\" column\n",
    "cleaned_d_dimers_stats = dataset['D-DIMERS_cleaned'].describe()\n",
    "\n",
    "dataset.insert(D_DIMERS_column_index, 'D-DIMERS_cleaned', dataset.pop('D-DIMERS_cleaned'))\n",
    "\n",
    "# Identifying and setting the highest value of D-DIMERS_cleaned to missing\n",
    "highest_ddimers_row = dataset['D-DIMERS_cleaned'].idxmax(skipna=True)\n",
    "dataset.loc[highest_ddimers_row, 'D-DIMERS_cleaned'] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the \"FIO2 eisagwgh\" column by keeping only the numeric part from the entries\n",
    "FIO2_eisagwgh_column_index = dataset.columns.get_loc('FIO2 eisagwgh')\n",
    "def clean_fio2(value):\n",
    "    if isinstance(value, str):\n",
    "        # Extract the numeric part from the string\n",
    "        match = re.search(r'\\d+', value)\n",
    "        return float(match.group()) if match else None\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "dataset['FIO2 eisagwgh_cleaned'] = dataset['FIO2 eisagwgh'].apply(clean_fio2)\n",
    "\n",
    "# Converting the cleaned column to numeric type\n",
    "dataset['FIO2 eisagwgh_cleaned'] = pd.to_numeric(dataset['FIO2 eisagwgh_cleaned'], errors='coerce')\n",
    "\n",
    "# Checking for missing values after cleaning and conversion\n",
    "cleaned_fio2_missing_values = dataset['FIO2 eisagwgh_cleaned'].isnull().sum()\n",
    "\n",
    "# Dropping the original 'FIO2 eisagwgh' column\n",
    "dataset.drop('FIO2 eisagwgh', axis=1, inplace=True)\n",
    "\n",
    "# Displaying basic descriptive statistics of the cleaned \"FIO2 eisagwgh\" column\n",
    "cleaned_fio2_stats = dataset['FIO2 eisagwgh_cleaned'].describe()\n",
    "\n",
    "dataset.insert(FIO2_eisagwgh_column_index, 'FIO2 eisagwgh_cleaned', dataset.pop('FIO2 eisagwgh_cleaned'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSC_column_index = dataset.columns.get_loc('GSC')\n",
    "# Treating entries with 'Δ' in the \"GSC\" column as missing values\n",
    "dataset['GSC_cleaned'] = dataset['GSC'].replace('Δ', None)\n",
    "\n",
    "# Converting the cleaned \"GSC\" column to a numeric type\n",
    "dataset['GSC_cleaned'] = pd.to_numeric(dataset['GSC_cleaned'], errors='coerce')\n",
    "\n",
    "# Checking for missing values after cleaning and conversion\n",
    "cleaned_gsc_missing_values = dataset['GSC_cleaned'].isnull().sum()\n",
    "\n",
    "# Dropping the original 'GSC' column\n",
    "dataset.drop('GSC', axis=1, inplace=True)\n",
    "\n",
    "# Displaying basic descriptive statistics of the cleaned \"GSC\" column\n",
    "cleaned_gsc_stats = dataset['GSC_cleaned'].describe()\n",
    "\n",
    "dataset.insert(GSC_column_index, 'GSC_cleaned', dataset.pop('GSC_cleaned'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the position of the original \"BP\" column\n",
    "bp_column_index = dataset.columns.get_loc('BP')\n",
    "\n",
    "# Splitting the \"BP\" column into two separate columns for systolic and diastolic values\n",
    "bp_split = dataset['BP'].str.split('/', expand=True)\n",
    "dataset['Systolic_BP'] = pd.to_numeric(bp_split[0], errors='coerce')\n",
    "dataset['Diastolic_BP'] = pd.to_numeric(bp_split[1], errors='coerce')\n",
    "\n",
    "# Dropping the original \"BP\" column\n",
    "dataset.drop('BP', axis=1, inplace=True)\n",
    "\n",
    "# Inserting the new systolic and diastolic BP columns in the position where the original BP column was\n",
    "dataset.insert(bp_column_index, 'Diastolic_BP', dataset.pop('Diastolic_BP'))\n",
    "dataset.insert(bp_column_index, 'Systolic_BP', dataset.pop('Systolic_BP'))\n",
    "\n",
    "# Replacing the unusually high value in 'Systolic_BP' with NaN\n",
    "dataset.loc[dataset['Subject_ID'] == 367, 'Systolic_BP'] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tINTUBATION_column_index = dataset.columns.get_loc('tINTUBATION')\n",
    "\n",
    "# Applying the cleaning process for \"tINTUBATION\"\n",
    "def clean_tintubation(value):\n",
    "    if isinstance(value, str):\n",
    "        # Extract the numeric part from the string\n",
    "        match = re.search(r'\\d+', value)\n",
    "        return float(match.group()) if match else None\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "dataset['tINTUBATION_cleaned'] = dataset['tINTUBATION'].apply(clean_tintubation)\n",
    "\n",
    "# Converting the cleaned column to numeric type\n",
    "dataset['tINTUBATION_cleaned'] = pd.to_numeric(dataset['tINTUBATION_cleaned'], errors='coerce')\n",
    "\n",
    "# Checking for missing values after cleaning and conversion\n",
    "cleaned_tintubation_missing_values = dataset['tINTUBATION_cleaned'].isnull().sum()\n",
    "\n",
    "# Dropping the original tINTUBATION column\n",
    "dataset.drop('tINTUBATION', axis=1, inplace=True)\n",
    "\n",
    "dataset.insert(tINTUBATION_column_index, 'tINTUBATION_cleaned', dataset.pop('tINTUBATION_cleaned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([0, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace \"ΑΛΛΕΡΓΙΑ\" with 0 in the original dataframe\n",
    "dataset[\"REMDESIVIR_Processed\"] = dataset[\"REMDESIVIR_Processed\"].replace(\"ΑΛΛΕΡΓΙΑ\", 0)\n",
    "\n",
    "# Now convert the \"REMDESIVIR_Processed\" column to numeric, coercing any errors into NaN\n",
    "dataset[\"REMDESIVIR_Processed\"] = pd.to_numeric(dataset[\"REMDESIVIR_Processed\"], errors='coerce')\n",
    "\n",
    "# Ensure there are no NaN values left in the \"REMDESIVIR_Processed\" column\n",
    "remaining_nan_values = dataset[\"REMDESIVIR_Processed\"].isnull().sum()\n",
    "remaining_nan_values, dataset[\"REMDESIVIR_Processed\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing implausible pH values with NaN\n",
    "dataset.loc[dataset['PH'] == 36.10, 'PH'] = pd.NA\n",
    "dataset.loc[dataset['PH'] == 73.41, 'PH'] = pd.NA\n",
    "# Converting the values in 'PH' column that are below 7 to missing values (NaN)\n",
    "dataset.loc[dataset['PH'] < 7, 'PH'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Στήλη6              58.811040\n",
       "qSOFA               21.443737\n",
       "FERRITIN            18.259023\n",
       "FIBRINOGEN          17.409766\n",
       "TnI                 12.951168\n",
       "D-DIMERS_cleaned    10.191083\n",
       "DIRECT BIL           9.766454\n",
       "APTT                 6.369427\n",
       "INR                  6.157113\n",
       "BIL                  5.520170\n",
       "K                    5.520170\n",
       "WBC                  5.307856\n",
       "CRP                  4.458599\n",
       "PH                   4.033970\n",
       "HCO3                 3.821656\n",
       "PCO2                 3.609342\n",
       "PO2                  2.972399\n",
       "NA                   2.972399\n",
       "LDH                  2.972399\n",
       "Glu                  2.760085\n",
       "ABSLYMPHOCYTES       2.760085\n",
       "UREA                 2.547771\n",
       "PLT                  2.547771\n",
       "Hb                   2.547771\n",
       "PERNEUTROPHILS       2.335456\n",
       "PERLYMPHOCYTES       2.335456\n",
       "SGOT                 2.335456\n",
       "CR                   2.335456\n",
       "SGPT                 2.335456\n",
       "CPK                  2.335456\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the percentage of missing values per column\n",
    "missing_values_percentage = dataset.isnull().mean() * 100\n",
    "\n",
    "# Display the results\n",
    "missing_values_percentage_sorted = missing_values_percentage.sort_values(ascending=False)\n",
    "missing_values_percentage_sorted.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Extracting the columns of interest from the dataset\\ndata_of_interest = dataset[columns_of_interest]\\n\\n# Checking for missing values\\nmissing_values = data_of_interest.isnull().sum()\\nmissing_values[missing_values > 0]  # Display only columns with missing values\\n#for col in columns_of_interest:\\n#    data_of_interest[col] = pd.to_numeric(data_of_interest[col])\\nfor col in columns_of_interest:\\n    data_of_interest.loc[:, col] = pd.to_numeric(data_of_interest[col])\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_columns = ['Subject_ID',\t'SAMPLE No',\t'Στήλη6', 'ΑΜ ΡΙΟΥ']\n",
    "dataset1_columns = [col for col in dataset if col not in index_columns]\n",
    "dataset1 = dataset[dataset1_columns]\n",
    "# Columns of interest for imputation\n",
    "columns_of_interest = [\n",
    "    \"AGE\", \"GENDER\", \"LOS\", \"DAYS OF SYMPTOMS\", \"INTUBATION\", \"tINTUBATION_cleaned\", \"CPAP\",\n",
    "    \"tCPAP\", \"HIGH FLOW\", \"t HIGH FLOW\", \"GSC_cleaned\", \"Systolic_BP\", \"Diastolic_BP\", \"PULSE RATE\", \n",
    "    \"TEMP\", \"PH\", \"PO2\", \"PCO2\", \"HCO3\", \"FIO2 eisagwgh_cleaned\", \"WHO score\", \"WBC\", \"LDH\", \n",
    "    \"CPK\", \"CRP\", \"FERRITIN\", \"PERNEUTROPHILS\", \"PERLYMPHOCYTES\", \"ABSLYMPHOCYTES\", \"Hb\", \n",
    "    \"PLT\", \"INR\", \"APTT\", \"FIBRINOGEN\", \"D-DIMERS_cleaned\", \"K\", \"NA\", \"Glu\", \"UREA\", \"CR\", \n",
    "    \"BIL\", \"DIRECT BIL\", \"SGOT\", \"SGPT\", \"TnI\", \"qSOFA\", \"Outcome_numerical\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the columns of interest from the dataset and creating a copy to avoid SettingWithCopyWarning\n",
    "data_of_interest = dataset1[columns_of_interest].copy()\n",
    "\n",
    "# Checking for missing values\n",
    "missing_values = data_of_interest.isnull().sum()\n",
    "missing_values[missing_values > 0]  # Display only columns with missing values\n",
    "\n",
    "# Converting columns to numeric, handling errors by converting them to NaN\n",
    "for col in columns_of_interest:\n",
    "    data_of_interest.loc[:, col] = pd.to_numeric(data_of_interest[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define discrete and continuous columns \n",
    "discrete_columns = ['GENDER', 'INTUBATION', 'CPAP', 'HIGH FLOW', 'WHO score', \"qSOFA\", \"Outcome_numerical\"]\n",
    "discrete_data = data_of_interest[discrete_columns]\n",
    "continuous_columns = [col for col in columns_of_interest if col not in discrete_columns]\n",
    "continuous_data = data_of_interest[continuous_columns] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "# Create the IterativeImputer with RandomForestRegressor as the estimator\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=5, max_depth= None, random_state=42),\n",
    "                            max_iter=100, random_state=42)\n",
    "\n",
    "# Perform the imputation\n",
    "data_imputed = imputer.fit_transform(continuous_data)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed_df = pd.DataFrame(data_imputed, columns=continuous_columns)\n",
    "\n",
    "data_imputed_df.index = dataset.index\n",
    "\n",
    "data_imputed_df.index = dataset1.index\n",
    "\n",
    "# Replace the original continuous columns in the dataset with the imputed data\n",
    "for column in continuous_columns:\n",
    "    dataset[column] = data_imputed_df[column]  \n",
    "    dataset1.loc[:, column] = data_imputed_df[column]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute missing values using RandomForestClassifier for discrete data\n",
    "def impute_numerical_discrete_rf(data, column, other_columns):\n",
    "    # Prepare the training data (where column is not missing)\n",
    "    train = data[data[column].notnull()]\n",
    "    test = data[data[column].isnull()]\n",
    "\n",
    "    if not test.empty:\n",
    "        X_train = train[other_columns]\n",
    "        y_train = train[column].astype('int')  # Ensure the target is integer\n",
    "        X_test = test[other_columns]\n",
    "\n",
    "        # Initialize and train classifier\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and fill missing values\n",
    "        predicted_values = clf.predict(X_test)\n",
    "        data.loc[data[column].isnull(), column] = predicted_values\n",
    "\n",
    "    return data\n",
    "\n",
    "# List of other columns to use as predictors; typically all other columns except the one being imputed\n",
    "other_columns = [col for col in dataset1.columns if col not in discrete_columns]\n",
    "\n",
    "# Apply the imputation for each discrete column\n",
    "for col in discrete_columns:\n",
    "    dataset1 = impute_numerical_discrete_rf(dataset1, col, other_columns)\n",
    "    dataset[col] = dataset1[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Applying KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_imputed = knn_imputer.fit_transform(data_of_interest)\n",
    "\n",
    "# Creating a DataFrame from the imputed data\n",
    "data_imputed_df = pd.DataFrame(data_imputed, columns=columns_of_interest)\n",
    "\n",
    "# Checking if any missing values remain\n",
    "missing_values_after_imputation = data_imputed_df.isnull().sum()\n",
    "missing_values_after_imputation[missing_values_after_imputation > 0]  # Display only columns with remaining missing values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_imputed_df.index = dataset.index\n",
    "# Columns to be replaced with imputed data\n",
    "columns_to_replace = data_imputed_df.columns\n",
    "# Replace the original columns in the dataset with the imputed columns\n",
    "for column in columns_to_replace:\n",
    "    dataset[column] = data_imputed_df[column]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting progress \n",
    "file_path_dataset13 = '/Complete_data.xlsx' \n",
    "dataset.to_excel(file_path_dataset13, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging the Dataset with CCI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ΑΜ ΡΙΟΥ    1\n",
       " CCI        0\n",
       " dtype: int64,\n",
       " 0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file2_path = '/Users/michael/Thesis project/Dataset/arxeio CCI.xlsx'\n",
    "dataset_CCI = pd.read_excel(file2_path)\n",
    "\n",
    "# Keeping rows where 'SAMPLE No' is not missing for each duplicate group in 'ΑΜ ΡΙΟΥ'\n",
    "# First, sorting by 'SAMPLE No' so that NaNs go to the end, then dropping duplicates while keeping the first\n",
    "data_sorted = dataset_CCI.sort_values(by=['ΑΜ ΡΙΟΥ', 'SAMPLE No'], na_position='last')\n",
    "dataset_CCI = data_sorted.drop_duplicates(subset='ΑΜ ΡΙΟΥ', keep='first')\n",
    "\n",
    "dataset_CCI = dataset_CCI.drop(columns=['SAMPLE No',\t'ΠΕΙΡΑΜΑ',\t'FACs'])\n",
    "\n",
    "final_merge_missing_values = dataset_CCI.isnull().sum()\n",
    "final_merge_duplicates = dataset_CCI.duplicated().sum()\n",
    "final_merge_missing_values, final_merge_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two tables\n",
    "dataset = pd.merge(dataset, dataset_CCI, on='ΑΜ ΡΙΟΥ', how='left')\n",
    "\n",
    "# Locate the row with 'Subject_ID' 497 and update the 'CCI' value to 3, this is the only replicated value\n",
    "dataset.loc[dataset['Subject_ID'] == 497, 'CCI'] = 3\n",
    "dataset.loc[dataset['Subject_ID'] == 370, 'Στήλη6'] = 'FACs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting progress \n",
    "file_path_dataset = '/preprocessed_COVID19_dataset_single.xlsx' \n",
    "dataset.to_excel(file_path_dataset, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging with Cytokines & Cytometry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the filter to include 'FACs'\n",
    "filtered_data_corrected = dataset[dataset['Στήλη6'].isin([1, 'FACs'])]\n",
    "\n",
    "# Saving the correctly filtered data to a new sheet in the Excel file\n",
    "output_file_path_corrected = '/filtered_COVID19_dataset_with_Cyto_corrected.xlsx'\n",
    "with pd.ExcelWriter(output_file_path_corrected, engine='xlsxwriter') as writer:\n",
    "    dataset.to_excel(writer, sheet_name='Original Data', index=False)\n",
    "    filtered_data_corrected.to_excel(writer, sheet_name='Cyto', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the additional datasets\n",
    "file_path_cytometry = '/all_data_cytometry_patients.xlsx'\n",
    "file_path_cytokines = '/Cytokines_all_patients.xlsx'\n",
    "file_path_cyto = '/filtered_COVID19_dataset_with_Cyto_corrected.xlsx'\n",
    "cyto_data = pd.read_excel(file_path_cyto, sheet_name='Cyto')\n",
    "cytometry_data = pd.read_excel(file_path_cytometry)\n",
    "# Drop 'Column1'\n",
    "cytometry_data.drop(columns='Column1', inplace=True)\n",
    "cytokines_data = pd.read_excel(file_path_cytokines)\n",
    "\n",
    "# Merging the datasets\n",
    "merged1 = pd.merge(cytokines_data,  cytometry_data, on='SAMPLE No', how='left')\n",
    "merged_Cyto = pd.merge(cyto_data, merged1, on='SAMPLE No', suffixes=('_cytometry', '_cytokines'), how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['Στήλη6', 'SAMPLE No',\t'ΑΜ ΡΙΟΥ'], inplace=True)\n",
    "merged_Cyto.drop(columns=['Στήλη6', 'SAMPLE No',\t'ΑΜ ΡΙΟΥ'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataFrame to a excel file\n",
    "exported_file_path = '/final_preprocessed_COVID19_dataset.xlsx' \n",
    "with pd.ExcelWriter(exported_file_path, engine='xlsxwriter') as writer:\n",
    "    dataset.to_excel(writer, sheet_name='Original Data', index=False)\n",
    "    merged_Cyto.to_excel(writer, sheet_name='Cyto', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
